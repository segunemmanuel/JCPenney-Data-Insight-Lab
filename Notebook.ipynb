{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tasks Performed</h1>\n",
    "The project embarked upon a comprehensive examination of diverse datasets provided by JCPenney, with the primary goal of extracting meaningful insights and understanding the intrinsic properties of the data. The datasets scrutinized included:\n",
    "\n",
    "- jcpenney_reviewers.json: This dataset concentrated on information pertaining to the reviewers.\n",
    "- jcpenney_products.json: It contained exhaustive details on various products.\n",
    "- products.csv: This was an aggregation of data related to products.\n",
    "- reviews.csv: It compiled reviews for an array of products.\n",
    "- users.csv: This dataset was a repository of user information.\n",
    "\n",
    "A methodical approach was used in this extensive data analysis report to extract insights from JCPenney's diverse datasets, notably `users.csv}, `reviews.csv}, and `jcpenney_reviewers.json}. The analysis followed a defined methodology that included data loading, preliminary investigation, data visualization, and data type classification.\n",
    "<h2>Data exploration</h2>\n",
    "\n",
    "**Data exploration and Initial Investigation:**\n",
    "The first step in the process for each dataset was to load the data into a pandas DataFrame and specify the file path. This essential first step prepared the ground for further examination.\n",
    "\n",
    "**Visualization for Initial Assessment:**\n",
    "Using pandas' `head()` and `tail()` functions, the data's initial structure and content were examined visually after loading. This two-pronged investigation made it possible to watch how data was presented at both the beginning and the end of the datasets, guaranteeing that the whole range of data was taken into account. The output was carefully formatted with different border colors, font tweaks, and backdrop modifications to improve readability.\n",
    "**Dimensionality:**\n",
    "The datasets' dimensions were explained, displaying the number of rows and columns that gave a general idea of the dataset's size. The comprehensive dataset details were printed, providing an understanding of the composition and integrity of the data. These details included data types and non-null value counts for every column.\n",
    "\n",
    "**Descriptive Statistics:**\n",
    "The distribution of the dataset was analyzed using descriptive statistics, which produced a quantitative summary that included features like central tendency, dispersion, and distribution shape. When it came to spotting trends and any abnormalities in the numerical data, these figures were essential.\n",
    "\n",
    "**Data Completeness Evaluation:**\n",
    "A comprehensive search for null values was conducted. Finding the missing data is essential for the data cleaning steps that follow and for guaranteeing the analysis's robustness.\n",
    "\n",
    "**Uniqueness and Categorization:**\n",
    "The datasets were examined closely to ensure that each column's values were unique. This stage was essential for figuring out how diverse the data were and for helping to distinguish between continuous and categorical variables. In order to handle unhashable types gracefully and guarantee a smooth analysis process, a try-except block was utilized.\n",
    "\n",
    "\n",
    "**Column/Variable Analysis:**\n",
    "Based on the data types in each column, the columns were categorized as either categorical or non-categorical, with an emphasis on differentiating between different object or category kinds. This categorization established the foundation for customized analytical methods based on the type of data in each column.\n",
    "\n",
    "<h2>Data visualization</h2>\n",
    "The data visualization process implemented for the users.csv dataset emphasized a meticulous examination of the unique values within the data, the geographical distribution of users across different states, and the  disstribution of users in the top states. Initially, the data was prepared by converting date-related columns to appropriate datetime formats to facilitate temporal analysi. A bar chart showing the count of unique values per columns of the datasets provided insights into the variety and potential categorization of the variables. This was followed by a detailed state distribution analysis through a bar chart, which shed light on user demographics and potential market segments. In order to provide more insightss, i finished the  visualization  with a pie chart that highlighted the top 10 states by user proportion. \n",
    "\n",
    "\n",
    "<h2>Data validation</h2>\n",
    "The goal of the {products.csv} dataset data validation process was to find and fix missing data. The dataset was first loaded, and to determine whether the data was complete, a thorough check for null values was made. This initial examination revealed missing values in a number of different columns. The dataset was then streamlined for improved data integrity and reliability during a cleaning phase in which all rows containing null values were eliminated. The successful elimination of all missing data was validated by a last validation check, guaranteeing that the dataset was now free of null values and better suited for precise and thorough data analysis. \n",
    "The quality of the dataset was improved by this meticulous approach to data validation, opening the door for more reliable and significant insights in future analysis.\n",
    "\n",
    "<h2>Data visualization</h2>\n",
    "After a thorough data analysis of the {reviews.csv} dataset, several visualizations were made to explore the characteristics of product reviews. To visualize the distribution of review scores and effectively highlight the frequency of each score category, the process started with the creation of a count plot. This provided insightful information about trends and preferences in customer satisfaction. An attribute called \"Review Length\" was added to the data in order to better understand its length for each review. Making use of this, a histogram was plotted along with a kernel density estimate to display the distribution of review lengths and identify patterns in the expression of customer feedback. After that, the visualization's main goal was to determine which ten products had received the most reviews. To improve clarity, this involved tallying distinct product IDs and combining them with matching product names. The resultant bar chart gave consumers a clear visual picture of these products and highlighted areas where they were very engaged or interested. A similar strategy was also used to highlight the top 10 users according to the quantity of reviews they had submitted. These users were displayed using a bar chart, which highlights the most engaged members of the review community. When taken as a whole, these visualizations provided a comprehensive understanding of the review data, including quantitative metrics like review counts and lengths as well as qualitative elements like review quality.\n",
    "This comprehensive visualization played a pivotal role in understanding consumer behavior, product performance, and overall user engagement within the dataset.\n",
    "\n",
    "\n",
    "<h2>Data analysis</h2>\n",
    "The analysis involved evaluating customer reviews from the jcpenney_products.json dataset using NLTK's VADER sentiment intensity analyzer. A structured DataFrame containing review texts and their distinct identifiers was produced by loading and processing the data. Based on its sentiment score, a custom function that used VADER categorized each review into positive, negative, or neutral categories. The number of reviews in each sentiment category was then calculated by averaging this sentiment data. A visually appealing bar chart showing the distribution of sentiments among the reviews and annotated with the number of reviews in each sentiment category was the end result of the analysis. This thorough method yielded a concise sentiment summary of the dataset's customer reviews. After that, the list was placed into a brand-new JSON file called \"reviews_with_tone.json.\" The first ten entries of the newly generated file were loaded into a pandas DataFrame and styled for improved readability, demonstrating the integration of sentiment analysis results into the initial review data, in order to confirm the process's success.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT ALL NEEDED LIBRARIES\n",
    "import pandas as pd\n",
    "import seaborn as sns  # Importing the seaborn library for data visualization\n",
    "import json  # Importing the json library for working with JSON data\n",
    "import matplotlib.pyplot as plt  # [Warning] Duplicate import statement\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Importing the CountVectorizer class from sklearn for text analysis\n",
    "import nltk  # Importing the nltk library for natural language processing tasks\n",
    "from nltk.corpus import stopwords  # Importing the stopwords corpus from nltk for text analysis\n",
    "from nltk.probability import FreqDist  # Importing the FreqDist class from nltk for frequency distribution analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  # Importing the SentimentIntensityAnalyzer class from nltk for sentiment analysis\n",
    "from nltk.tokenize import word_tokenize  # Importing the word_tokenize function from nltk for tokenizing text\n",
    "import warnings  # Importing the warnings module for handling warnings\n",
    "warnings.filterwarnings('ignore')  # Ignoring warning messages during execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## <b>1.0 <span style='color:#B21010'></span>Data exploration</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specifying the path to the users.csv file\n",
    "dataPath = 'users.csv'\n",
    "\n",
    "# Reading the users.csv file into a dataframe\n",
    "dfUsers = pd.read_csv(dataPath)\n",
    "\n",
    "# Printing the first few rows of the users.csv file using the head() function\n",
    "print(\"First Few Rows of users.csv using head():\")\n",
    "\n",
    "# Styling the first 10 rows of the dfUsers dataframe for better visualization\n",
    "styledHeadDf = dfUsers.head(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': 'black',\n",
    "        'font-size': '10px',\n",
    "        'width':'20%',\n",
    "    }\n",
    ")\n",
    "styledHeadDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas' 'tail()' function, we show the dataset's final 10 rows for a cursory analysis.\n",
    "# This aids in giving a brief overview of the dataset, including with column names and values.\n",
    "print(\"Last 10 rows of users.csv using tail():\")\n",
    "\n",
    "# Styling the last 10 rows of the dfUsers dataframe for better visualization\n",
    "styledTailDf = dfUsers.tail(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid black',\n",
    "        'color': 'white',\n",
    "        'background-color': 'grey',\n",
    "        'font-size': '10px',\n",
    "        'width':'20%',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Outputting the styled dataframe\n",
    "styledTailDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the number of rows and columns in the users.csv dataset\n",
    "print(f\"\\nThe users.csv dataset has {dfUsers.shape[0]} rows and {dfUsers.shape[1]} columns.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing dataset information\n",
    "print(\"Dataset Info:\\n\")\n",
    "# Printing the information about the dfUsers dataframe using the info() function\n",
    "print(dfUsers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing descriptive statistics for the users.csv dataset\n",
    "print(\"\\nDescriptive Statistics for users.csv:\")\n",
    "\n",
    "# Calculating the descriptive statistics for the dfUsers dataframe using the describe() function\n",
    "statsDescriptionForUsers = dfUsers.describe().style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': 'grey',\n",
    "        'font-size': '10px',\n",
    "        'width':'20%',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Outputting the styled descriptive statistics dataframe\n",
    "statsDescriptionForUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of missing (null) values in each column of the dataset is determined and shown by this code block.\n",
    "# The 'isnull()' function in pandas is used to find nulls in the DataFrame, and then'sum()' is used to aggregate the nulls column-wise.\n",
    "# To find and properly manage missing data, this check is essential throughout the preprocessing stage of data.\n",
    "print(\"Null Values in each column:\")\n",
    "print(dfUsers.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of unique values in each column, handling unhashable types\n",
    "# In the code block below an iteration through each columns of every dataset is done using pandas' \n",
    "# 'nunique()' to count unique values. A try-except block handles  unhashable types like lists or dictionaries,\n",
    "#  outputting a custom message in such cases.  This helps in assessing data variability and identifying potential categorical columns.\n",
    "  \n",
    "print(\"Number of unique values in each column:\")\n",
    "for col in dfUsers.columns:\n",
    "    try:\n",
    "        print(f\"{col}: {dfUsers[col].nunique()}\")\n",
    "    except TypeError:\n",
    "        print(f\"{col}: Unhashable type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and non-categorical columns.\n",
    "# The purpose of this code below  is to divide the dataset's columns into categories and non-categorical data types. Using pandas''select_dtypes' function, it first attempts to locate categorical columns (those with data types of 'object' or 'category'). List comprehension is used to manually identify 'object' type columns as categorical in the event that a TypeError is raised (due to unhashable types in columns). Next, the 'object' and 'category' data types are excluded in order to identify the non-categorical columns.\n",
    "# The function ends  by printing lists of both category and non-categorical columns. This gives a clear picture of the structure of the dataset, which is crucial for further data analysis procedures.\n",
    "try:\n",
    "    categorical_cols = dfUsers.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "except TypeError:\n",
    "    categorical_cols = [col for col in dfUsers.columns if dfUsers[col].dtype == 'object']\n",
    "non_categorical_cols = dfUsers.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Columns:\")\n",
    "print(categorical_cols)\n",
    "print(\"\\nNon-Categorical Columns:\")\n",
    "print(non_categorical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews.csv\n",
    "dataPath = 'reviews.csv' \n",
    "dfReviews = pd.read_csv(dataPath)\n",
    "# Displaying the top few rows of the dataset for preliminary examination using pandas',\n",
    "#  'head()' method. The structure of the dataset, including column names and beginning values,\n",
    "#  is briefly summarized using this technique.\n",
    "#  Verifying data loading and comprehending dataset layout are crucial aspects of data analysis.\n",
    "print(\"First 10 rows of reviews.csv using head():\")\n",
    "styledHeadDfReviews = dfReviews.head(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': 'black',\n",
    "        'font-size': '10px',\n",
    "#         'width':'10%',\n",
    "        'border':'1px,1px,1px,1px'\n",
    "    }\n",
    ")\n",
    "# Output the head()\n",
    "styledHeadDfReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the last 10  rows of the dataset for preliminary examination using pandas's,\n",
    "#  'tail()' method. The structure of the dataset, including column names and values,\n",
    "#  is briefly summarized using this technique.\n",
    "#  Verifying data loading and comprehending dataset layout are crucial aspects of data analysis.\n",
    "print(\"Last 10 rows of reviews.csv using tail():\")\n",
    "styledTailDfReviews = dfReviews.tail(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid black',\n",
    "        'color': 'white',\n",
    "        'background-color': 'grey',\n",
    "        'font-size': '10px',\n",
    "    }\n",
    ")\n",
    "styledTailDfReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows and columns in the dataset using the the shape() \n",
    "# method from pandas,the shape method returns back a tuple that reflects the DataFrame's dimensions.\n",
    "print(f\"\\nThe reviews.csv dataset has {dfReviews.shape[0]} rows and {dfUsers.shape[1]} columns.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code block below provides comprehensive dataset details,\n",
    "# such as column data types and non-null counts using the info method from pandas\n",
    "print(\"Dataset Info:\\n\")\n",
    "print(dfReviews.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code block produces descriptive statistics for the dataset,\n",
    "#  including max, min, and standard deviation. \n",
    "# This is accomplished by using the 'describe()' function, and transposing the result for easier reading.\n",
    "# The DataFrame is also styled for improved visual appeal, including border radius, border style, text color, font size, and background color.\n",
    "\n",
    "print(\"\\nDescriptive Statistics for reviews.csv:\")\n",
    "statsDescriptionForReviews = dfReviews.describe().style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': '#900C3F',\n",
    "        'font-size': '10px',\n",
    "        'width':'20%',\n",
    "    }\n",
    ")\n",
    "# print(styled_df)\n",
    "statsDescriptionForReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of missing (null) values in each column of the dataset is determined and shown by this code block.\n",
    "# The 'isnull()' function in pandas is used to find nulls in the DataFrame, and then'sum()' is used to aggregate the nulls column-wise.\n",
    "print(\"Null Values in each column:\")\n",
    "print(dfReviews.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the number of unique values in each column, handling unhashable types\n",
    "# In the code block below an iteration through each columns of every dataset is done using pandas' \n",
    "#'nunique()' to count unique values. A try-except block handles  unhashable types like lists or dictionaries, \n",
    "#outputting a custom message in such cases.  This helps in assessing data variability and identifying potential categorical columns.\n",
    "print(\"Number of unique values in each column:\")\n",
    "for col in dfReviews.columns:\n",
    "    try:\n",
    "        print(f\"{col}: {dfReviews[col].nunique()}\")\n",
    "    except TypeError:\n",
    "        print(f\"{col}: Unhashable type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and non-categorical columns, handling unhashable types\n",
    "# The purpose of this code below  is to divide the dataset's columns into categories and non-categorical data types.\n",
    "# Using pandas''select_dtypes' function, it first attempts to locate categorical columns (those with data types of 'object' or 'category').\n",
    "# List comprehension is used to manually identify 'object' type columns as categorical in the event that a TypeError \n",
    "#is raised (due to unhashable types in columns). Next, the 'object' and 'category' data types are excluded in order to \n",
    "# identify the non-categorical columns.\n",
    "# The function ends  by printing lists of both category and non-categorical columns. \n",
    "#This gives a clear picture of the structure of the dataset, which is crucial for further data analysis procedures.\n",
    "try:\n",
    "    categorical_cols = dfReviews.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "except TypeError:\n",
    "    categorical_cols = [col for col in dfReviews.columns if dfReviews[col].dtype == 'object']\n",
    "non_categorical_cols = dfReviews.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Columns:\")\n",
    "print(categorical_cols)\n",
    "print(\"\\nNon-Categorical Columns:\")\n",
    "print(non_categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the top 10 rows of the dataset for preliminary examination using pandas' 'head()' method. \n",
    "# The structure of the dataset, including column names and  values, is briefly summarized using this technique.\n",
    "#  Verifying data loading and comprehending dataset layout are crucial aspects of data analysis.\n",
    "# The output of the dataframe is styled via  set_properties\n",
    "dataPath = 'jcpenney_reviewers.json'  \n",
    "dfReviewers = pd.read_json(dataPath,lines=\"true\")\n",
    "print(\"First 10  rows of jcpenney_reviewers.json using head():\")\n",
    "dfReviewersStyled = dfReviewers.head(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': '#AA4A44',\n",
    "        'font-size': '10px',\n",
    "        'width':'10%',\n",
    "    }\n",
    ")\n",
    "# Output\n",
    "dfReviewersStyled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the last 10 rows of the dataset for preliminary examination using pandas' 'tail()' method. \n",
    "# The structure of the dataset, including column names and  values, is briefly summarized using this technique.\n",
    "#  Verifying data loading and comprehending dataset layout are crucial aspects of data analysis.\n",
    "# The output of the dataframe is also styled via  set_properties\n",
    "print(\"Last 10  rows of jcpenney_reviewers.json using tail():\")\n",
    "dfReviewersStyledTail = dfReviewers.tail(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'black',\n",
    "        'background-color': '#DAF7A6',\n",
    "        'font-size': '10px',\n",
    "        'width':'10%',\n",
    "    }\n",
    ")\n",
    "# output\n",
    "dfReviewersStyledTail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows and columns in the dataset using the the shape() method from pandas,\n",
    "# the shape method returns back a tuple that reflects the DataFrame's dimensions.\n",
    "print(f\"\\nThe dataset has {dfReviewers.shape[0]} rows and {dfReviewers.shape[1]} columns.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code block below provides comprehensive dataset details,\n",
    "# such as column data types and non-null counts using the info method from pandas\n",
    "print(\"JcPenney Reviewers(json) Info:\\n\")\n",
    "print(dfReviewers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code block produces descriptive statistics for the datasets, \n",
    "# including max, min, and standard deviation. \n",
    "# This is accomplished by using the 'describe()' function, and transposing the result for easier reading.\n",
    "# The DataFrame is also styled for improved visual appeal, including border radius, border style, text color, font size, and background color.\n",
    "print(\"\\nDescriptive Statistics for jcpenney_reviewers.json:\")\n",
    "productJsonReviews= dfReviewers.describe().style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': '#FFC300',\n",
    "        'font-size': '10px',\n",
    "        'width':'10%',\n",
    "    }\n",
    ")\n",
    "# print(styled_df)\n",
    "productJsonReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of missing (null) values in each column of the dataset is determined and shown by this code block.\n",
    "# The 'isnull()' function in pandas is used to find nulls in the DataFrame, and then'sum()' is used to aggregate the nulls column-wise.\n",
    "# To find and properly manage missing data, this check is essential throughout the preprocessing stage of data.\n",
    "print(\"Null Values in each column:\")\n",
    "print(dfReviewers.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of unique values in each column, handling unhashable types\n",
    "# In the code block below an iteration through each columns of every dataset is done using pandas' \n",
    "# 'nunique()' to count unique values. A try-except block handles  unhashable types like lists or dictionaries,\n",
    "#  outputting a custom message in such cases.  This helps in assessing data variability and identifying potential categorical columns.\n",
    "print(\"Number of unique values in each column:\")\n",
    "for col in dfReviewers.columns:\n",
    "    try:\n",
    "        print(f\"{col}: {dfReviewers[col].nunique()}\")\n",
    "    except TypeError:\n",
    "        print(f\"{col}: Unhashable type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and non-categorical columns.\n",
    "# The purpose of this code below  is to divide the dataset's columns into categories and non-categorical data types. Using pandas''select_dtypes' function, it first attempts to locate categorical columns (those with data types of 'object' or 'category'). List comprehension is used to manually identify 'object' type columns as categorical in the event that a TypeError is raised (due to unhashable types in columns). Next, the 'object' and 'category' data types are excluded in order to identify the non-categorical columns.\n",
    "# The function ends  by printing lists of both category and non-categorical columns. This gives a clear picture of the structure of the dataset, which is crucial for further data analysis procedures.\n",
    "try:\n",
    "    categorical_cols = dfReviewers.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "except TypeError:\n",
    "    categorical_cols = [col for col in dfReviewers.columns if dfReviewers[col].dtype == 'object']\n",
    "non_categorical_cols = dfReviewers.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Categorical Columns:\")\n",
    "print(categorical_cols)\n",
    "print(\"\\nNon-Categorical Columns:\")\n",
    "print(non_categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.0\"></a>\n",
    "### <b>2.0 <span style='color:#B21010'></span> Data Visualization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distibution of categorical and non categorical variables for users.csv\n",
    "# users.csv,since users.csv has no null values we can proceed to visualizing the rest of the data \n",
    "# as shon in tyhe EDA Null Values in each column in the Users dataset:\n",
    "# Username    0\n",
    "# DOB         0\n",
    "# State       0\n",
    "# dtype: int64\n",
    "# Reload the dataset due to previous variable clearance and convert DOB to datetime format\n",
    "# Let's correct the function based on the provided snippet and integrate all visualizations.\n",
    "\n",
    "\n",
    "# Convert the 'DOB' column in the dfUsers DataFrame to datetime format. \n",
    "# Set errors='coerce' to replace any invalid dates with NaT (Not a Time) value.\n",
    "dfUsers['DOB'] = pd.to_datetime(dfUsers['DOB'], errors='coerce')\n",
    "\n",
    "\n",
    "# Function to visualize data in the given DataFrame.\n",
    "# Takes two parameters: df - the DataFrame, ds_name - the name of the dataset.\n",
    "def visualize_data(df, ds_name):\n",
    "\n",
    "# This purpose of the code is to visualize data on users.csv. \n",
    "# The method generates a figure of a given size after printing the title for the visualization\n",
    "# It determines how many distinct values there are in each column of the users.csv and displays these values as a bar graph. \n",
    "# After the graph has labels for the axes and a title, it is displayed.\n",
    "\n",
    "# Print title for the data visualization\n",
    "    print(f\"Data Visualization for {ds_name} Dataset\\n{'-'*30}\")\n",
    "# Create a figure with a specific size\n",
    "    plt.figure(figsize=(15, 7))  \n",
    "# Calculate the number of unique values per column\n",
    "    n_unique = df.nunique()\n",
    "# Plot a bar graph to visualize the number of unique values per column\n",
    "    n_unique.plot(kind='bar')\n",
    "# Set the title and labels for the graph\n",
    "    plt.title(\"Number of Unique Values per Column\")\n",
    "    plt.ylabel(\"Number of Unique Values\")\n",
    "    plt.xticks(rotation=45)\n",
    "# Display the graph\n",
    "    plt.show()\n",
    "       \n",
    "\n",
    "# The code below's goal is to display the user distribution according to each state.\n",
    "#  It generates a bar graph with the states shown on the x-axis and the number of users on the y-axis.\n",
    "#  Title and axis labels are applied to the graph. To avoid overlapping, the x-axis labels are rotated by 90 degrees using the rotation=90 parameter.\n",
    "#    In order to modify the layout and avoid label overlap, i used the plt.tight_layout() function.\n",
    "#  The graph is finally shown.\n",
    "\n",
    "# Create a figure with a specific size of 15 inches and 8 width\n",
    "    plt.figure(figsize=(15, 8))\n",
    "# Count the number of occurrences of each state in the 'State' column and plot a bar graph\n",
    "    df['State'].value_counts().plot(kind='bar', color='teal')\n",
    "# Set the title and labels for the graph\n",
    "    plt.title('State Distribution of Users')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Number of Users')\n",
    "    plt.xticks(rotation=90)\n",
    "# Adjust the layout to prevent overlapping of labels and display the graph\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# This code block's role is to examine the top 10 states' categorical data according to the percentage of users in the dataset. \n",
    "# It generates a pie chart, with each slice denoting the percentage of users from a given state among all users. \n",
    "# Using the autopct='%1.1f%%' argument, the percentage value is shown on each slice. \n",
    "# The y-axis label is eliminated and the graph is given a title. The pie chart is finally displayed.\n",
    "\n",
    "# Perform categorical data analysis on the top 10 states based on the proportion of users\n",
    "    plt.figure(figsize=(20, 12))\n",
    "# Count the number of occurrences of each state in the 'State' column and select the top 10 states\n",
    "    df['State'].value_counts().head(10).plot(kind='pie', autopct='%1.1f%%')\n",
    "# Set the title and remove the y-axis label\n",
    "    plt.title('Top 10 States Proportion of Users')\n",
    "    plt.ylabel('')\n",
    "# Display the pie chart\n",
    "    plt.show()\n",
    "\n",
    "# Call the visualization function with the user data\n",
    "visualize_data(dfUsers, 'Users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.0\"></a>\n",
    "<b>3.0 <span style='color:#B21010'></span> Data Validation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block below  eliminates null values from the df_products DataFrame.\n",
    "# The dataset is reloaded again from the 'products.csv' file.\n",
    "# The isnull().sum() method is used to do the initial check, which counts the sum  of null values in each column.\n",
    "#  Dropna() is used to eliminate rows with null values, resulting in the creation of a new DataFrame called df_products_cleaned.\n",
    "# After cleaning, a last check is made to determine how many null values there are.\n",
    "# The console displays the outcomes of the first and last checks.\n",
    "\n",
    "# Reload the dataset due to the execution state reset\n",
    "df_products = pd.read_csv('products.csv')\n",
    "\n",
    "# Initial check for null values before cleaning\n",
    "null_values_before = df_products.isnull().sum()\n",
    "print(\"Null values before cleaning:\")\n",
    "print(null_values_before)\n",
    "# Remove rows with null values\n",
    "df_products_cleaned = df_products.dropna()\n",
    "# Final check for null values after cleaning\n",
    "null_values_after = df_products_cleaned.isnull().sum()\n",
    "print(\"\\nNull values after cleaning:\")\n",
    "print(null_values_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.0\"></a>\n",
    "<b>4.0 <span style='color:#B21010'>||</span> Data Visualization for reviews.csv</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_reviews = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Visualization for Score Distribution\n",
    "# This code uses a countplot to illustrate the distribution of review scores. \n",
    "# The figure is made using the countplot function from the sns module, and the x-axis variable is the 'Score' column from the df_reviews DataFrame. \n",
    "# A title, axis labels, and a figure size adjustment are applied to the plot to improve visibility. \n",
    "# The frequency of each score category is displayed in the resulting graphic.\n",
    "plt.figure(figsize=(22, 12))\n",
    "sns.countplot(x='Score', data=df_reviews, palette='viridis')\n",
    "plt.title('Distribution of Review Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Find the review length\n",
    "df_reviews['Review Length'] = df_reviews['Review'].str.len()\n",
    "\n",
    "# Visualization for Review Length Distribution\n",
    "# This code uses a histogram to display the distribution of review lengths. \n",
    "# The 'Review Length' column from the df_reviews DataFrame is used as the data in the plot, which is made using the histplot function from the sns module.\n",
    "# To regulate the granularity of the histogram, the number of bins is set to thirty. A title, axis labels, and a figure size adjustment are applied to the plot to improve visibility.\n",
    "# The frequency of review lengths within each bin is displayed in the resulting figure along with an overlaying kernel density estimate (kde) curve.\n",
    "\n",
    "plt.figure(figsize=(22, 12))\n",
    "sns.histplot(df_reviews['Review Length'], bins=30, color='orange', kde=True)\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Top 10 Products with the Most Reviews\n",
    "# Using a count of each unique 'Uniq_id' in the 'Uniq_id' column of the df_reviews DataFrame, \n",
    "# this method determines the top 10 products with the most reviews. \n",
    "# The relevant product names are obtained by merging the top product IDs with the df_products DataFrame.\n",
    "#  Next, using kind='bar', the names are shown as a bar graph. In order to improve readability,\n",
    "#  the x-axis labels are rotated, and the graph is given a title and axis names.\n",
    "top_products = df_reviews['Uniq_id'].value_counts().head(10).index\n",
    "# Merge to get the product names using the Uniq_id\n",
    "top_products_with_names = df_products[df_products['Uniq_id'].isin(top_products)]\n",
    "# Ensure that 'Uniq_id' is set as the index in the top_products_with_names dataframe\n",
    "top_products_with_names.set_index('Uniq_id', inplace=True)\n",
    "# Map the names to the top_products series\n",
    "top_product_names = top_products.map(top_products_with_names['Name'])\n",
    "# Now plot with product names\n",
    "plt.figure(figsize=(22, 12))\n",
    "top_product_names.value_counts().plot(kind='bar', color='blue')\n",
    "plt.title('Top 10 Products with the Most Reviews')\n",
    "plt.xlabel('Product Name')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Top 10 Users with the Most Reviews\n",
    "# The top 10 users with the most reviews are displayed using this code. \n",
    "# The 'Username' column of the df_reviews DataFrame is where the value_counts() method is used to count the occurrences of each unique username.\n",
    "#  Next, using kind='bar,' the counts are shown as a bar graph. \n",
    "# In order to improve readability, the x-axis labels are rotated, and the graph is given a title and axis names.\n",
    "top_users = df_reviews['Username'].value_counts().head(10)\n",
    "plt.figure(figsize=(22, 12))\n",
    "top_users.plot(kind='bar', color='green')\n",
    "plt.title('Top 10 Users with the Most Reviews')\n",
    "plt.xlabel('Username')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.0\"></a>\n",
    "<b>5.0 <span style='color:#B21010'> </span> Data analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NLTK's VADER sentiment intensity analyzer, this code analyzes customer reviews from jcpenney_products.json for sentiment.\n",
    "# The SentimentIntensityAnalyzer() class from NLTK is used to initialize the VADER sentiment intensity analyzer.\n",
    "# The 'jcpenney_products.json' file is read line by line into a list of dictionaries via a with open() block.\n",
    "# Using a list comprehension, the data is flattened to produce a list of tuples (uniq_id, review_text).\n",
    "# The flattened data is placed into a DataFrame called df_reviews, with columns called \"uniq_id\" and \"review_text.\"\n",
    "# To apply sentiment analysis to every review content, a function called analyze_sentiment is defined.\n",
    "# The sentiment score is determined using the VADER analyzer, and the sentiment is categorized as \"positive,\" \"negative,\" or \"neutral\" depending on the compound score.\n",
    "# The apply() method is used to apply the analyze_sentiment function to the'review_text' column of the df_reviews DataFrame.\n",
    "# The sentiment result is then placed in a new column called'sentiment'.\n",
    "# Using the value_counts() method on the'sentiment' column of the df_reviews DataFrame, the number of each sentiment category is tallied, \n",
    "# and the result is placed in the sentiment_counts variable.\n",
    "\n",
    "\n",
    "# Initialize NLTK's VADER sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Assuming the JSON data is stored in a file called 'products_reviews.jsonl'\n",
    "with open('jcpenney_products.json', 'r') as file:\n",
    "    data = [json.loads(line) for line in file.readlines()]\n",
    "\n",
    "# Flatten the data to get a list of tuples (uniq_id, review_text)\n",
    "reviews_data = [(item['uniq_id'], review['Review']) for item in data for review in item['Reviews']]\n",
    "\n",
    "# Create a DataFrame from the reviews\n",
    "df_reviews = pd.DataFrame(reviews_data, columns=['uniq_id', 'review_text'])\n",
    "\n",
    "# Define a function to apply sentiment analysis\n",
    "def analyze_sentiment(review_text):\n",
    "    score = sia.polarity_scores(review_text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply the function to get sentiment for each review\n",
    "df_reviews['sentiment'] = df_reviews['review_text'].apply(analyze_sentiment)\n",
    "\n",
    "# Count the number of each sentiment\n",
    "sentiment_counts = df_reviews['sentiment'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "# This code creates a bar chart to show the reviews' sentiment distribution. \n",
    "# It generates a bar plot with distinct colors for every sentiment category using the plot function from the sentiment_counts DataFrame. \n",
    "# Next, axis labels, rotation of the x-axis label, and a title are added to customize the plot.\n",
    "# Lastly, each bar has annotations added to it that show how many reviews belong to each sentiment group. \n",
    "# The show function is used to display the generated plot.\n",
    "# Plot the sentiment distribution\n",
    "plt.figure(figsize=(22, 12))  # Create a new figure with a specific size\n",
    "ax = sentiment_counts.plot(kind='bar', color=['green', 'gray', 'red'])  # Create a bar plot from sentiment_counts DataFrame\n",
    "plt.title('Sentiment Analysis of Reviews')  # Set the title of the plot\n",
    "plt.xlabel('Sentiment')  # Set the label for the x-axis\n",
    "plt.ylabel('Number of Reviews')  # Set the label for the y-axis\n",
    "plt.xticks(rotation=0)  # Set the rotation of x-axis labels to 0 degrees\n",
    "\n",
    "# Adding annotations\n",
    "for p in ax.patches:  # Iterate over each bar patch in the plot\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))  # Add the height of each bar as an annotation\n",
    "\n",
    "# Display the graph\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews are categorized using the code block below, and they are then saved in a new file for additional examination.\n",
    "# It starts by setting up the VADER sentiment intensity analyzer for NLTK.\n",
    "# Next, the 'jcpenney_products.json' file's JSON data is loaded.\n",
    "# The function \"get_sentiment\" is then defined; it accepts a text parameter and returns the sentiment classification determined by the VADER analysis.\n",
    "# The method thereafter iterates over every item in the dataset and every review included in the item's 'Reviews' list.\n",
    "# The sentiment categorization for each review is obtained by calling the 'get_sentiment' function, and it is then added to the review's 'Tone' attribute.\n",
    "#  the altered data is saved to a brand-new JSON file named \"modified_reviews.json.\"\n",
    "#Ultimately, the new file is read in order to view the newly added field\n",
    "\n",
    "# Initialize NLTK's VADER sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the JSON data\n",
    "with open('jcpenney_products.json', 'r') as file:\n",
    "    data = [json.loads(line) for line in file.readlines()]\n",
    "\n",
    "# Function to classify sentiment\n",
    "def get_sentiment(text):\n",
    "    score = sia.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Extracting reviews and their sentiments\n",
    "review_data = []\n",
    "for item in data:\n",
    "    for review in item['Reviews']:\n",
    "        review_tone = get_sentiment(review['Review'])\n",
    "        review_data.append({'Review': review['Review'], 'Tone': review_tone})\n",
    "\n",
    "# Save the review data to a new JSON file\n",
    "with open('reviews_with_tone.json', 'w') as outfile:\n",
    "    json.dump(review_data, outfile, indent=4)\n",
    "\n",
    "# read the head of the new file\n",
    "try:\n",
    "    # Attempt to read the file as a standard JSON array of objects\n",
    "    newFile = pd.read_json('reviews_with_tone.json')\n",
    "except ValueError:\n",
    "    # If there is a ValueError, attempt to read it as line-delimited JSON\n",
    "    newFile = pd.read_json('reviews_with_tone.json', lines=True)\n",
    "# Get the first 10 rows of the file and apply styling\n",
    "outputOfNewModifiedFile = newFile.head(10).style.set_properties(\n",
    "    **{\n",
    "        'border': '1.3px solid white',\n",
    "        'color': 'white',\n",
    "        'background-color': 'black',\n",
    "        'font-size': '10px',\n",
    "    }\n",
    ")\n",
    "# output\n",
    "outputOfNewModifiedFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.memory_usage.html\n",
    "- https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html#pandas.DataFrame.isnull\n",
    "- https://www.analyticsvidhya.com/blog/2021/06/top-15-pandas-data-exploration-functions/\n",
    "- https://canvas.stir.ac.uk/courses/13894/pages/functions-with-arguments?module_item_id=738358"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
